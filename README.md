# Unconstrained Local Optimization Algorithms Comparison using Scipy Library Optimizers

This repository represents an analysis of unconstrained optimization on multiple convex and non-convex test functions using some of Scipy library optimizers. Visual illustrations of the test functions are provided for 2-dimensional cases while summary reports are created when analyzing the performance over multiple n-dimensional cases. The listed optimizers are selected such that they require at least no information about the test function.

<br><br>

## Background
Mathematical optimization is a broad topic that aims to answer questions relevant to the problem of finding numerical minimum or maximum. It can be overwhelming when trying to solve an optimization function without knowing its characteristics that can help identify the best approach for finding the solution. Additionally, categorizing a certain problem to fall into the optimization categories that can be solved efficiently may dictate changing its form which is not always straightforward and intuitive. For that reason, some use black-box optimization as a recourse for solving optimization problems [[1]](#1). 


Black-box optimization or usually referred to as derivative-free optimization includes approaches for solving optimization problems where the structure of objective function and its constraints is unknown, unexploitable, or non-existent. The hierarchy of such algorithms is not consistent across multiple sources. 


However, an easy hierarchical categorization is obtained as the following [[2]](#2): 

- **Local Optimization**
    - First-Order
        - Conjugate Gradient (CG)
    - Second-Order 
        - Truncated Newton's Method (TNC) 
        - Newton-CG
    - Quasi-Newton
        - Broyden-Fletcher-Goldfarb-Shanno (BFGS)
    - Direct Method
        - Powell's Method
        - Nelder-Mead 


    First-order methods rely on the cost functions gradient while Second-order methods need the second-order derivative Hessian matrix to determine the descent direction and step size towards the next iteration. However, some methods can approximate first and/or second-order information numerically making them derivative-free. Those methods are CG and BFGS which will be used in this analysis with Powell and Nelder-Mead methods. The implementation of first-order and quasi-Newton in Scipy allows providing the gradients to improve the performance of the optimizer. Similarly, the performance of second-order methods can be improved by providing the Hessian matrix if available.    

<br>

- **Global Optimization**

    Global optimization methods include approaches that are more suitable when the local minimum or maximum is not unique or the uniqueness is unknown. The categories mentioned under local optimization include methods that can be robust against getting stuck in a local optimum (e.g., Adam optimizer which is a first-order method). It is just the mentioned algorithms above that can easily get stuck in a local optimum. Below are some methods that are mentioned but not included in this repository analysis. 

    - Stochastic Methods
        - Simulated Annealing
        - Mesh Adaptive Direct Search (MADS) 
    - Population Methods
        - Evolution Strategy (ES)
        - Genetic Algorithm (GA)

<br><br>

## Methodology 
Only the mentioned local methods above are used in the presented analysis. The analysis is performed by using the following test functions [[2]](#2):   

___
### Rosenbrock's Valley 
$$f(x_1, x_2) = (a - x_1)^2 + b(x_2-x_1^2)^2 $$
where 
- $x_1, x_2, b, \text{ and } a \in \mathbb{R}  $

Another variant is considered when dealing with more than two variables: 
$$ f( \mathbf{x}) = \sum_{i=1}^{N-1} \left(100(x_{i+1} - x_i^2 )^2 + (1-x_i)^2 \right)$$
where 
- $ \mathbf{x} = (x_1, ..., x_N) \in \mathbb{R}^N$

___
### Quadratic Function
$$ f(\mathbf{x}) = \mathbf{x^TAx + c^Tx} $$ 
where
- $\mathbf{x} \in \mathbb{R}^N $.
- $\mathbf{A} $ is a positive-definite matrix.

The ill-conditioned form of $\mathbf{A} $ is generated by setting the difference between the largest and smallest eigenvalue to be large. The well-conditioned case uses an identity matrix [[3]](#3).  
___
### Wheeler's Ridge

$$ f(x_1, x_2) = -e^{\left(-(x_1x_2 - a)^2 - (x_2 - a)^2 \right)} $$
where
- $x_1, x_2, \text{ and } a \in \mathbb{R}  $

___
### Michalewicz Function
$$ f( \mathbf{x}) = -\sum_{i=1}^d sin(x_i) sin^{2m}\left(\frac{ix_i^2}{\pi}\right) $$ 
where 
- $  \mathbf{x}= (x_1, ..., x_d) \in \mathbb{R}^d$
- $m \in \mathbb{R}$

___

Only Rosenbrock's valley and the quadratic function are used for analyzing the performance over 2-dimensional problems. The remaining are used for illustrations of the steps taken by the selected optimization algorithms. For more test functions check  [[2]](#2) and [[4]](#4). 
<br><br>

## Results

### Rosenbrock's Valley
![Alt text](<images/Rosenbrock's Valley.gif>) 
___
### Quadratic Function
#### Well-conditioned $A$ matrix
![Alt text](<images/Well-Conditioned Quadratic Function.gif>) 


#### Ill-conditioned $A$ matrix
![Alt text](<images/Ill-Conditioned Quadratic Function.gif>) 

___
### Wheeler's Ridge
![Alt text](<images/Wheeler's Ridge Function.gif>)

___
### Michalewicz Function
![Alt text](<images/Michalewicz Function.gif>)

___
### Multi-Dimensional Analysis

<table border="1" class="dataframe">
  <thead>
    <tr>
      <th>Test</th>
      <th colspan="6" halign="left">Rosenbrock's valley</th>
      <th colspan="6" halign="left">Ill-conditioned quadratic</th>
      <th colspan="6" halign="left">Well-conditioned quadratic</th>
    </tr>
    <tr>
      <th>Dimension</th>
      <th>8</th>
      <th>16</th>
      <th>32</th>
      <th>64</th>
      <th>128</th>
      <th>256</th>
      <th>8</th>
      <th>16</th>
      <th>32</th>
      <th>64</th>
      <th>128</th>
      <th>256</th>
      <th>8</th>
      <th>16</th>
      <th>32</th>
      <th>64</th>
      <th>128</th>
      <th>256</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Nelder-Mead</th>
      <td>638</td>
      <td>2503</td>
      <td>5433</td>
      <td>11612</td>
      <td>24304</td>
      <td>50511</td>
      <td>838</td>
      <td>2428</td>
      <td>5351</td>
      <td>11683</td>
      <td>24559</td>
      <td>53262</td>
      <td>429</td>
      <td>2482</td>
      <td>5480</td>
      <td>11687</td>
      <td>24571</td>
      <td>50540</td>
    </tr>
    <tr>
      <th>Powell</th>
      <td>19</td>
      <td>23</td>
      <td>29</td>
      <td>46</td>
      <td>48</td>
      <td>79</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>6</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>CG</th>
      <td>226</td>
      <td>272</td>
      <td>496</td>
      <td>895</td>
      <td>541</td>
      <td>722</td>
      <td>292</td>
      <td>47</td>
      <td>80</td>
      <td>164</td>
      <td>142</td>
      <td>155</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>BFGS</th>
      <td>40</td>
      <td>71</td>
      <td>140</td>
      <td>192</td>
      <td>261</td>
      <td>449</td>
      <td>11</td>
      <td>16</td>
      <td>23</td>
      <td>45</td>
      <td>54</td>
      <td>63</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
    </tr>
  </tbody>
</table>

<br><br>

## References
<a id="1">[1]</a> Boyd, Stephen P., and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.

<a id="2">[2]</a> Kochenderfer, Mykel J., and Tim A. Wheeler. Algorithms for optimization. MIT Press, 2019.

<a id="3">[3]</a> Piggott, Matthew, et al. “Ill-Conditioning and Roundoff Errors.” Primer on Programming in Python and Mathematical/Computational Techniques for Scientists and Engineers, edited by James Percival, Imperial College London, 2020, [Link](https://primer-computational-mathematics.github.io/book/c_mathematics/numerical_methods/14_ill_conditioning_errors.html). 


<a id="4">[4]</a> Surjanovic, S. & Bingham, D. (2013). Virtual Library of Simulation Experiments: Test Functions and Datasets. Retrieved July 10, 2023, [Link](https://www.sfu.ca/~ssurjano/optimization.html).


<a id="5">[5]</a> Varoquaux, Gaël. Mathematical optimization: finding minima of functions. Scipy Lecture notes. 2023, [Link](https://scipy-lectures.org/advanced/mathematical_optimization/index.html).







 









